<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>cplint on SWISH Manual</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>
<div id="header">
<h1 class="title"><code>cplint</code> on SWISH Manual</h1>
</div>
<div id="TOC">
<ul>
<li><a href="#syn">Syntax</a><ul>
<li><a href="#discrete">Discrete Probability Distributions</a><ul>
<li><a href="#problog-syntax">ProbLog Syntax</a></li>
<li><a href="#prism-syntax">PRISM Syntax</a></li>
</ul></li>
<li><a href="#cont">Continuous Probability Densities</a><ul>
<li><a href="#dc">Distributional Clauses Syntax</a></li>
</ul></li>
</ul></li>
<li><a href="#semantics">Semantics</a></li>
<li><a href="#inf">Inference</a><ul>
<li><a href="#uncondq">Unconditional Queries</a></li>
<li><a href="#condq">Conditional Queries on Discrete Variables</a></li>
<li><a href="#condqcont">Conditional Queries on Continuous Variables</a></li>
<li><a href="#graphing">Graphing the Results</a></li>
<li><a href="#parameters">Parameters</a></li>
</ul></li>
<li><a href="#learning">Learning</a><ul>
<li><a href="#input">Input</a><ul>
<li><a href="#preamble">Preamble</a></li>
<li><a href="#background-and-initial-lpadcpl-program">Background and Initial LPAD/CPL-program</a></li>
<li><a href="#language-bias">Language Bias</a></li>
<li><a href="#example-interpretations">Example Interpretations</a></li>
</ul></li>
<li><a href="#commands">Commands</a><ul>
<li><a href="#parameter-learning">Parameter Learning</a></li>
<li><a href="#structure-learning">Structure Learning</a></li>
</ul></li>
<li><a href="#parameters-1">Parameters</a></li>
</ul></li>
<li><a href="#download-query-results-through-an-api">Download Query Results through an API</a></li>
<li><a href="#manual-in-pdf">Manual in PDF</a></li>
<li><a href="#bibliography">Bibliography</a></li>
</ul>
</div>
<h1 id="syn">Syntax</h1>
<p><code>cplint</code> permits the definition of discrete probability distributions and continuous probaility densities.</p>
<h2 id="discrete">Discrete Probability Distributions</h2>
<p>LPAD and CP-logic programs consist of a set of annotated disjunctive clauses. Disjunction in the head is represented with a semicolon and atoms in the head are separated from probabilities by a colon. For the rest, the usual syntax of Prolog is used. A general CP-logic clause has the form</p>
<pre><code>h1:p1 ; ... ; hn:pn :- Body.</code></pre>
<p>where <code>Body</code> is a conjunction of goals as in Prolog. No parentheses are necessary. The <code>pi</code> are numeric expressions. It is up to the user to ensure that the numeric expressions are legal, i.e. that they sum up to less than one.</p>
<p>If the clause has an empty body, it can be represented like this</p>
<pre><code>h1:p1 ; ... ; hn:pn.</code></pre>
<p>If the clause has a single head with probability 1, the annotation can be omitted and the clause takes the form of a normal prolog clause, i.e.</p>
<pre><code>h1 :- Body.</code></pre>
<p>stands for</p>
<pre><code>h1:1 :- Body.</code></pre>
<p>The coin example of <span class="citation">[<a href="#ref-VenVer04-ICLP04-IC">19</a>]</span> is represented as (file <a href="http://cplint.lamping.unife.it/example/inference/coin.cpl"><code>coin.cpl</code></a>)</p>
<pre><code>heads(Coin):1/2 ; tails(Coin):1/2 :- 
  toss(Coin),\+biased(Coin).

heads(Coin):0.6 ; tails(Coin):0.4 :- 
  toss(Coin),biased(Coin).

fair(Coin):0.9 ; biased(Coin):0.1.

toss(coin).</code></pre>
<p>The first clause states that if we toss a coin that is not biased it has equal probability of landing heads and tails. The second states that if the coin is biased it has a slightly higher probability of landing heads. The third states that the coin is fair with probability 0.9 and biased with probability 0.1 and the last clause states that we toss a coin with certainty.</p>
<p>Moreover, the bodies of rules may contain built-in predicates, predicates from the libraries <code>lists</code>, <code>apply</code> and <code>clpr/nf_r</code> plus the predicate</p>
<pre><code>average/2</code></pre>
<p>that, given a list of numbers, computes its arithmetic mean.</p>
<p>The body of rules may also contain the predicate <code>prob/2</code> that computes the probability of an atom, thus allowing nested probability computations. For example (<a href="http://cplint.lamping.unife.it/example/inference/meta.pl"><code>meta.pl</code></a>)</p>
<pre><code>a:0.2:-
  prob(b,P),
  P&gt;0.2.</code></pre>
<p>is a valid rule.</p>
<p>Moreover, the probabilistic annotations can be variables, as in (<a href="http://cplint.lamping.unife.it/example/inference/flexprob.pl"><code>flexprob.pl</code></a>))</p>
<pre><code>red(Prob):Prob.

draw_red(R, G):-
  Prob is R/(R + G),
  red(Prob).</code></pre>
<p>Variables in probabilistic annotations must be ground when resolution reaches the end of the body, otherwise an exception is raised.</p>
<p>Alternative ways of specifying probability distribution include</p>
<pre><code>A:discrete(Var,D):-Body.</code></pre>
<p>or</p>
<pre><code>A:finite(Var,D):-Body.</code></pre>
<p>where <code>A</code> is an atom containg variable <code>Var</code> and <code>D</code> is a list of couples <code>Value:Prob</code> assigning probability <code>Prob</code> to <code>Value</code>. Moreover, you can use</p>
<pre><code>A:uniform(Var,D):-Body.</code></pre>
<p>where <code>A</code> is an atom containg variable <code>Var</code> and <code>D</code> is a list of values each taking the same probability (1 over the length of <code>D</code>).</p>
<h3 id="problog-syntax">ProbLog Syntax</h3>
<p>You can also use ProbLog <span class="citation">[<a href="#ref-DBLP:conf/ijcai/RaedtKT07">7</a>]</span> syntax, so a general clause takes the form</p>
<pre><code>p1::h1 ; ... ; pn::hn :- Body</code></pre>
<p>where the <code>pi</code> are numeric expressions.</p>
<h3 id="prism-syntax">PRISM Syntax</h3>
<p>You can also use PRISM <span class="citation">[<a href="#ref-DBLP:conf/ijcai/SatoK97">17</a>]</span> syntax, so a program is composed of a set of regular Prolog rules whose body may contain calls to the <code>msw/2</code> predicate (multi-ary switch). A call <code>msw(term,value)</code> means that a random variable associated to <code>term</code> assumes value <code>value</code>. The admissible values for a discrete random variable are specified using facts for the <code>values/2</code> predicate of the form</p>
<pre><code>values(T,L).</code></pre>
<p>where <code>T</code> is a term (possibly containing variables) and <code>L</code> is a list of values. The distribution over values is specified using directives for <code>set_sw/2</code> of the form</p>
<pre><code>:- set_sw(T,LP).</code></pre>
<p>where <code>T</code> is a term (possibly containing variables) and <code>LP</code> is a list of probability values. Remember that in PRISM each call to <code>msw/2</code> refers to a different random variable, i.e., no memoing is performed, differently from the case of LPAD/CP-Logic/ProbLog.</p>
<p>For example, the coin example above in PRISM syntax becomes (<a href="http://cplint.lamping.unife.it/example/inference/coinmsw.pl"><code>coinmsw.pl</code></a>)</p>
<pre><code>values(throw(_),[heads,tails]).
:- set_sw(throw(fair),[0.5,0.5]).
:- set_sw(throw(biased),[0.6,0.4]).
values(fairness,[fair,biased]).
:- set_sw(fairness,[0.9,0.1]).
res(Coin,R):- toss(Coin),fairness(Coin,Fairness),msw(throw(Fairness),R).
fairness(_Coin,Fairness) :- msw(fairness,Fairness).
toss(coin).</code></pre>
<h2 id="cont">Continuous Probability Densities</h2>
<p><code>cplint</code> handles continuous random variables as well with its sampling inference module. To specify a probability density on an argument <code>Var</code> of an atom <code>A</code> you can used rules of the form</p>
<pre><code>A:Density:- Body</code></pre>
<p>where <code>Density</code> is a special atom identifying a probability density on variable <code>Var</code> and <code>Body</code> (optional) is a regular clause body. Allowed <code>Density</code> atoms are</p>
<ul>
<li><p><code>uniform(Var,L,U)</code>: <code>Var</code> is uniformly distributed in <span class="math inline">\([L,U]\)</span></p></li>
<li><p><code>gaussian(Var,Mean,Variance)</code>: <code>Var</code> follows a Gaussian distribution with mean <code>Mean</code> and variance <code>Variance</code></p></li>
<li><p><code>dirichlet(Var,Par)</code>: <code>Var</code> is a list of real numbers following a Dirichlet distribution with <span class="math inline">\(\alpha\)</span> parameters specified by the list <code>Par</code></p></li>
<li><p><code>gamma(Var,Shape,Scale)</code> <code>Var</code> follows a gamma distribution with shape parameter <code>Shape</code> and scale parameter <code>Scale</code>.</p></li>
<li><p><code>beta(Var,Alpha,Beta)</code> <code>Var</code> follows a beta distribution with parameters <code>Alpha</code> and <code>Beta</code>.</p></li>
<li><p><code>poisson(Var,Lambda)</code> <code>Var</code> follows a Poisson distribution with parameter <code>Lambda</code>.</p></li>
</ul>
<p>For example</p>
<pre><code>g(X): gaussian(X,0, 1).</code></pre>
<p>states that argument <code>X</code> of <code>g(X)</code> follows a Gaussian distribution with mean 0 and variance 1.</p>
<p>For example, <a href="http://cplint.lamping.unife.it/example/inference/gaussian_mixture.pl"><code>gaussian_mixture.pl</code></a> defines a mixture of two Gaussians:</p>
<pre><code>heads:0.6;tails:0.4.
g(X): gaussian(X,0, 1).
h(X): gaussian(X,5, 2).
mix(X) :- heads, g(X).
mix(X) :- tails, h(X).</code></pre>
<p>The argument <code>X</code> of <code>mix(X)</code> follows a distribution that is a mixture of two Gaussian, one with mean 0 and variance 1 with probability 0.6 and one with mean 5 and variance 2 with probability 0.4.</p>
<p>The parameters of the distribution atoms can be taken from the probabilistic atom, the example <a href="http://cplint.lamping.unife.it/example/inference/gauss_mean_est.pl"><code>gauss_mean_est.pl</code></a></p>
<pre><code>value(I,X) :-
  mean(M),
  value(I,M,X).
mean(M): gaussian(M,1.0, 5.0).
value(_,M,X): gaussian(X,M, 2.0).</code></pre>
<p>states that for an index <code>I</code> the continuous variable <code>X</code> is sampled from a Gaussian whose variance is 2 and whose mean is sampled from a Guassian with mean 1 and variance 5.</p>
<p>Any operation is allowed on continuous random variables. The example below (<a href="http://cplint.lamping.unife.it/example/inference/kalman_filter.pl"><code>kalman_filter.pl</code></a>) encodes a Kalman filter:</p>
<pre><code>kf(N,O, T) :-
  init(S),
  kf_part(0, N, S,O,T).
kf_part(I, N, S,[V|RO], T) :-
  I &lt; N,
  NextI is I+1,
  trans(S,I,NextS),
  emit(NextS,I,V),
  kf_part(NextI, N, NextS,RO, T).
kf_part(N, N, S, [],S).
trans(S,I,NextS) :-
  {NextS =:= E + S},
  trans_err(I,E).
emit(NextS,I,V) :-
  {NextS =:= V+X},
  obs_err(I,X).
init(S):gaussian(S,0,1).
trans_err(_,E):gaussian(E,0,2).
obs_err(_,E):gaussian(E,0,1).</code></pre>
<p>Continuous random variables are involved in arithmetic expressions (in <code>trans/3</code> and <code>emit/3</code>). It is often convenient, as in this case, to use CLP(R) constraints (by including the directive <code>:- use_module(library(clpr)).</code>) as in this way the expressions can be used in multiple directions and the same clauses can be used both to sample and to evaluate the weight the sample on the basis of evidence, otherwise different clauses have to be written. In case random variables are not sufficiently instantiated to exploit expressions for inferring the values of other variables, inference will return an error.</p>
<h3 id="dc">Distributional Clauses Syntax</h3>
<p>You can also use the syntax of Distributional Clauses (DC) <span class="citation">[<a href="#ref-Nitti2016">12</a>]</span>. Continuous random variables are represented in this case by term whose distribution can be specified with density atoms as in</p>
<pre><code>T~Density&#39; := Body.</code></pre>
<p>Here <code>:=</code> replaces the implication symbol, <code>T</code> is a term and <code>Density'</code> is one of the density atoms above witthout the <code>Var</code> argument, because <code>T</code> itself represents a random variables. In the body of clauses you can use the infix operator <code>~=</code> to equate a term representing a random variable with a logical variable or a constant as in <code>T ~= X</code>. Internally <code>cplint</code> transforms the terms representing random variables into atoms with an extra argument for holding the variable. DC can be used to represent also discrete distributions using</p>
<pre><code>T~uniform(L) := Body.
T~finite(D) := Body.</code></pre>
<p>where <code>L</code> is a list of values and <code>D</code> is a list of couples <code>P:V</code> with <code>P</code> a probability and <code>V</code> a value. If <code>Body</code> is empty, as in regular Prolog, the implication symbol <code>:=</code> can be omitted.</p>
<p>The Indian GPA problem from <a href="http://www.robots.ox.ac.uk/~fwood/anglican/examples/viewer/?worksheet=indian-gpa" class="uri">http://www.robots.ox.ac.uk/~fwood/anglican/examples/viewer/?worksheet=indian-gpa</a>in distributional clauses syntax (<a href="https://github.com/davidenitti/DC/blob/master/examples/indian-gpa.pl" class="uri">https://github.com/davidenitti/DC/blob/master/examples/indian-gpa.pl</a>) takes the form (<a href="http://cplint.lamping.unife.it/example/inference/indian_gpadc.pl"><code>indian_gpadc.pl</code></a>):</p>
<pre><code>is_density_A:0.95;is_discrete_A:0.05.
% the probability distribution of GPA scores for American students is
% continuous with probability 0.95 and discrete with probability 0.05

agpa(A): beta(A,8,2) :- is_density_A.
% the GPA of American students follows a beta distribution if the
% distribution is continuous

american_gpa(G) : finite(G,[4.0:0.85,0.0:0.15]) :- is_discrete_A.
% the GPA of American students is 4.0 with probability 0.85 and 0.0
% with 
% probability 0.15 if the
% distribution is discrete
american_gpa(A):- agpa(A0), A is A0*4.0.
% the GPA of American students is obtained by rescaling the value of
% agpa
% to the (0.0,4.0) interval
is_density_I : 0.99; is_discrete_I:0.01.
% the probability distribution of GPA scores for Indian students is
% continuous with probability 0.99 and discrete with probability 
% 0.01
igpa(I): beta(I,5,5) :- is_density_I.
% the GPA of Indian students follows a beta distribution if the
% distribution is continuous
indian_gpa(I): finite(I,[0.0:0.1,10.0:0.9]):-  is_discrete_I.
% the GPA of Indian students is 10.0 with probability 0.9 and 0.0
% with
% probability 0.1 if the
% distribution is discrete
indian_gpa(I) :- igpa(I0), I is I0*10.0.
% the GPA of Indian students is obtained by rescaling the value 
% of igpa
% to the (0.0,4.0) interval
nation(N) : finite(N,[a:0.25,i:0.75]).
% the nation is America with probability 0.25 and India with 
% probability 0.75
student_gpa(G):- nation(a),american_gpa(G).
% the GPA of the student is given by american_gpa if the nation is 
% America
student_gpa(G) :- nation(i),indian_gpa(G).
% the GPA of the student is given by indian_gpa if the nation 
%is India</code></pre>
<p>See</p>
<h1 id="semantics">Semantics</h1>
<p>The semantics of LPADs for the case of programs without functions symbols can be given as follows. An LPAD defines a probability distribution over normal logic programs called <em>worlds</em>. A world is obtained from an LPAD by first grounding it, by selecting a single head atom for each ground clause and by including in the world the clause with the selected head atom and the body. The probability of a world is the product of the probabilities associated to the heads selected. The probability of a ground atom (the query) is given by the sum of the probabilities of the worlds where the query is true.</p>
<p>If the LPAD contains function symbols, the definition is more complex, see <span class="citation">[<a href="#ref-DBLP:journals/ai/Poole97">13</a>,<a href="#ref-Rig15-PLP-IW">16</a>,<a href="#ref-DBLP:journals/jair/SatoK01">18</a>]</span>.</p>
<p>For the semantics of programs with continuous random variables, see <span class="citation">[<a href="#ref-TLP:8688161">9</a>]</span> that defines the probability space for <span class="math inline">\(N\)</span> continuous random variables by considering the Borel <span class="math inline">\(\sigma\)</span>-algebra over <span class="math inline">\(\mathbb{R}^N\)</span> and defines a Lebesgue measure on this set as the probability measure. The probability space is lifted to cover the entire program using the least model semantics of constraint logic programs. Alternatively, <span class="citation">[<a href="#ref-Nitti2016">12</a>]</span> defines the semantics of distributional clauses by resorting to a stochastic <span class="math inline">\(Tp\)</span> operator. <code>cplint</code> allows more freedom than distributional clauses in the use of continuous random variables in expressions, for example <a href="http://cplint.lamping.unife.it/example/inference/kalman_filter.pl"><code>kalman_filter.pl</code></a> would not be allowed by distributional clauses.</p>
<h1 id="inf">Inference</h1>
<p><code>cplint</code> answers queries using the module <code>pita</code> or <code>mcintyre</code>. The first performs the program transformation technique of <span class="citation">[<a href="#ref-RigSwi10-ICLP10-IC">14</a>]</span>. Differently from that work, techniques alternative to tabling and answer subsumption are used. The latter performs approximate inference by sampling using a different program transformation technique and is described in <span class="citation">[<a href="#ref-Rig13-FI-IJ">15</a>]</span>. Only <code>mcintyre</code> is able to handle continuous random variables.</p>
<p>For answering queries, you have to prepare a Prolog file where you first load the inference module (for example <code>pita</code>), initialize it with a directive (for example <code>:- pita</code>) and then enclose the LPAD clauses in <code>:-begin_lpad.</code> or <code>:-begin_plp.</code> and <code>:-end_lpad.</code> or <code>:-end_plp.</code> For example, the coin program above can be stored in <a href="http://cplint.lamping.unife.it/example/inference/coin.pl"><code>coin.pl</code></a> for performing inference with <code>pita</code> as follows</p>
<pre><code>:- use_module(library(pita)).
:- pita.
:- begin_lpad.
heads(Coin):1/2 ; tails(Coin):1/2:- 
toss(Coin),\+biased(Coin).

heads(Coin):0.6 ; tails(Coin):0.4:- 
toss(Coin),biased(Coin).

fair(Coin):0.9 ; biased(Coin):0.1.

toss(coin).
:- end_lpad.</code></pre>
<p>The same program for <code>mcintyre</code> is</p>
<pre><code>:- use_module(library(mcintyre)).
:- mc.
:- begin_lpad.
heads(Coin):1/2 ; tails(Coin):1/2:- 
toss(Coin),\+biased(Coin).

heads(Coin):0.6 ; tails(Coin):0.4:- 
toss(Coin),biased(Coin).

fair(Coin):0.9 ; biased(Coin):0.1.

toss(coin).
:- end_lpad.</code></pre>
<p>You can have also (non-probabilistic) clauses outside <code>:-begin/end_lpad.</code> These are considered as database clauses. In <code>pita</code> subgoals in the body of probabilistic clauses can query them by enclosing the query in <code>db/1</code>. For example (<a href="http://cplint.lamping.unife.it/example/inference/testdb.pl"><code>testdb.pl</code></a>)</p>
<pre><code>:- use_module(library(pita)).
:- pita.
:- begin_lpad.
sampled_male(X):0.5:-
  db(male(X)).
:- end_lpad.
male(john).
male(david).</code></pre>
<p>You can also use <code>findall/3</code> on subgoals defined by database clauses (<a href="http://cplint.lamping.unife.it/example/inference/persons.pl"><code>persons.pl</code></a>)</p>
<pre><code>:- use_module(library(pita)).
:- pita.
:- begin_lpad.
male:M/P; female:F/P:-
  findall(Male,male(Male),LM),
  findall(Female,female(Female),LF),
  length(LM,M),
  length(LF,F),
  P is F+M.
:- end_lpad.
male(john).
male(david).
female(anna).
female(elen).
female(cathy).</code></pre>
<p>Aggregate predicates on probabilistic subgoals are not implemented due to their high computational cost (if the aggregation is over <span class="math inline">\(n\)</span> atoms, the values of the aggregation are potentially <span class="math inline">\(2^n\)</span>). The Yap version of <code>cplint</code> includes reasoning algorithms that allows aggregate predicates on probabilistic subgoals, see <a href="http://ds.ing.unife.it/~friguzzi/software/cplint/manual.html" class="uri">http://ds.ing.unife.it/~friguzzi/software/cplint/manual.html</a>.</p>
<p>In <code>mcintyre</code> you can query database clauses in the body of probabilistic clauses without any special syntax. You can also use <code>findall/3</code>.</p>
<p><code>cplint</code> on SWISH allows two types of programs: LPAD and Prolog. In the first, you create a new LPAD in the editor, in the latter a Prolog program.</p>
<p>In LPADs, you write in the editor the program without the library import and the compiler directives. <code>pita</code> is used for inference.</p>
<p>You ask queries by writing the atom of which you want to compute the probability. In the coin example, the probability of <code>heads(coin)</code> can be obtained with the</p>
<pre><code>?- heads(coin).</code></pre>
<p>In Prolog programs, you have to enter the program above (<code>coin.pl</code>).</p>
<h2 id="uncondq">Unconditional Queries</h2>
<p>The unconditional probability of an atom can be asked using <code>pita</code> with the predicate</p>
<pre><code>prob(:Query:atom,-Probability:float).</code></pre>
<p>as in</p>
<pre><code>?- prob(heads(coin),P).</code></pre>
<p>If the query is non-ground, <code>prob/2</code> returns in backtracking the succesful instantiations together with their probability.</p>
<p>When using <code>mcintyre</code>, the predicate for querying is</p>
<pre><code>mc_prob(:Query:atom,-Probability:float).</code></pre>
<p>as in</p>
<pre><code>?- mc_prob(heads(coin),P).</code></pre>
<p>With <code>mcintyre</code>, you can also take a given number of sample with</p>
<pre><code>mc_sample(:Query:atom,+Samples:int,-Successes:int,-Failures:int,
  -Probability:float).</code></pre>
<p>as in (<a href="http://cplint.lamping.unife.it/example/inference/coinmc.pl"><code>coinmc.pl</code></a>)</p>
<pre><code>?- mc_sample(heads(coin),1000,S,F,P).</code></pre>
<p>that samples <code>heads(coin)</code> 1000 times and returns in <code>S</code> the number of successes, in <code>F</code> the number of failures and in <code>P</code> the estimated probability (<code>S/1000</code>).</p>
<p>Differently from exact inference, in approximate inference the query can be a conjunction of atoms.</p>
<p>If you are just interested in the probability, you can use</p>
<pre><code>mc_sample(:Query:atom,+Samples:int,-Probability:float) </code></pre>
<p>as in (<a href="http://cplint.lamping.unife.it/example/inference/coinmc.pl"><code>coinmc.pl</code></a>)</p>
<pre><code>?- mc_sample(heads(coin),1000,Prob).</code></pre>
<p>that samples <code>heads(coin)</code> 1000 times and returns the estimated probability that a sample is true (i.e., that a sample succeeds).</p>
<p>Moreover, you can sample arguments of queries with</p>
<pre><code>mc_sample_arg(:Query:atom,+Samples:int,?Arg:var,-Values:list).</code></pre>
<p>The predicate samples <code>Query</code> a number of <code>Samples</code> times. <code>Arg</code> should be a variable in <code>Query</code>. The predicate returns in <code>Values</code> a list of couples <code>L-N</code> where <code>L</code> is the list of values of <code>Arg</code> for which <code>Query</code> succeeds in a world sampled at random and <code>N</code> is the number of samples returning that list of values. If <code>L</code> is the empty list, it means that for that sample the query failed. If <code>L</code> is a list with a single element, it means that for that sample the query is determinate. If, in all couples <code>L-N</code>, <code>L</code> is a list with a single element, it means that the clauses in the program are mutually exclusive, i.e., that in every sample, only one clause for each subgoal has the body true. This is one of the assumptions taken for programs of the PRISM system <span class="citation">[<a href="#ref-DBLP:journals/jair/SatoK01">18</a>]</span>. For example <a href="http://cplint.lamping.unife.it/example/inference/pfcglr.pl"><code>pfcglr.pl</code></a> and <a href="http://cplint.lamping.unife.it/example/inference/plcg.pl"><code>plcg.pl</code></a> satisfy this constraint while <a href="http://cplint.lamping.unife.it/example/inference/markov_chain.pl"><code>markov_chain.pl</code></a> and <a href="http://cplint.lamping.unife.it/example/inference/var_obj.pl"><code>var_obj.pl</code></a> donâ€™t.</p>
<p>An example of use of the above predicate is</p>
<pre><code>?- mc_sample_arg(reach(s0,0,S),50,S,Values). </code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/markov_chain.pl"><code>markov_chain.pl</code></a> that takes 50 samples of <code>L</code> in <code>findall(S,(reach(s0,0,S),L)</code>.</p>
<p>You can sample arguments of queries also with</p>
<pre><code>mc_sample_arg_first(:Query:atom,+Samples:int,?Arg:var,-Values:list).</code></pre>
<p>samples <code>Query</code> a number of <code>Samples</code> times and returns in <code>Values</code> a list of couples <code>V-N</code> where <code>V</code> is the value of <code>Arg</code> returned as the first answer by <code>Query</code> in a world sampled at random and <code>N</code> is the number of samples returning that value. <code>V</code> is failure if the query fails. <code>mc_sample_arg_first/4</code> differs from <code>mc_sample_arg/4</code> because the first just computes the first answer of the query for each sampled world.</p>
<p>Finally, you can compute expectations with</p>
<pre><code>mc_expectation(:Query:atom,+N:int,?Arg:var,-Exp:float).</code></pre>
<p>that computes the expected value of <code>Arg</code> in <code>Query</code> by sampling. It takes <code>N</code> samples of <code>Query</code> and sums up the value of <code>Arg</code> for each sample. The overall sum is divided by <code>N</code> to give <code>Exp</code>.</p>
<p>An example of use of the above predicate is</p>
<pre><code>?- mc_expectation(eventually(elect,T),1000,T,E).</code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/pctl_slep.pl"><code>pctl_slep.pl</code></a> that returns in <code>E</code> the expected value of <code>T</code> by taking 1000 samples.</p>
<h2 id="condq">Conditional Queries on Discrete Variables</h2>
<p>The conditional probability of an atom query given another atom evidence can be asked using <code>pita</code> with the predicate</p>
<pre><code>prob(:Query:atom,:Evidence:atom,-Probability:float).</code></pre>
<p>as in</p>
<pre><code>?- prob(heads(coin),biased(coin),P).</code></pre>
<p>If the query/evidence are non-ground, <code>prob/3</code> returns in backtracking ground instantiations together with their probability.</p>
<p>If the evidence is composed of more than one atom, a clause of the form</p>
<pre><code>evidence:- e1,...,en.</code></pre>
<p>should be added to the program, where <code>e1,...,en</code> are the evidence atoms, and the query <code>prob(goal,evidence,P).</code> should be asked.</p>
<p>When using <code>mcintyre</code>, you can ask conditional queries with rejection sampling or with Metropolis-Hastings Markov Chain Monte Carlo. In rejection sampling <span class="citation">[<a href="#ref-von195113">20</a>]</span>, you first query the evidence and, if the query is successful, query the goal in the same sample, otherwise the sample is discarded. In Metropolis-Hastings MCMC, <code>mcintyre</code> follows the algorithm proposed in <span class="citation">[<a href="#ref-nampally2014adaptive">11</a>]</span> (the non adaptive version). A Markov chain is built by building an initial sample and by generating successor samples.</p>
<p>The initial sample is built by randomly sampling choices so that the evidence is true. This is done with a backtracking meta-interpreter that starts with the goal and randomizes the order in which clauses are selected during the search so that the initial sample is unbiased. Each time the meta-interpreter encounters a probabilistic choice, it first checks whether a value has already been sampled, if not, it takes a sample and records it. If a failure is obtained, the meta-interpreter backtracks to other clauses but without deleting samples. Then the goal is queries using regular MCINTYRE.</p>
<p>A successor sample is obtained by deleting a fixed number (parameter <code>Lag</code>) of sampled probabilistic choices. Then the evidence is queried using regular MCINTYRE starting with the undeleted choices. If the query succeeds, the goal is queried using regular MCINTYRE. The sample is accepted with a probability of <span class="math inline">\(\min\{1,\frac{N_0}{N_1}\}\)</span> where <span class="math inline">\(N_0\)</span> is the number of choices sampled in the previous sample and <span class="math inline">\(N_1\)</span> is the number of choices sampled in the current sample. In <span class="citation">[<a href="#ref-nampally2014adaptive">11</a>]</span> the lag is always 1 but the proof in <span class="citation">[<a href="#ref-nampally2014adaptive">11</a>]</span> that the above acceptance probability yields a valid Metropolis-Hastings algorithm holds also when forgetting more than one sampled choice, so the lag is user defined in <code>cplint</code>.</p>
<p>Then the number of successes of the query is increased by 1 if the query succeeded in the last accepted sample. The final probability is given by the number of successes over the total number of samples.</p>
<p>You can take a given number of sample with rejection sampling using</p>
<pre><code>mc_rejection_sample(:Query:atom,:Evidence:atom,+Samples:int,
  -Successes:int,-Failures:int,-Probability:float).</code></pre>
<p>as in (<a href="http://cplint.lamping.unife.it/example/inference/coinmc.pl"><code>coinmc.pl</code></a>)</p>
<pre><code>?- mc_rejection_sample(heads(coin),biased(coin),1000,S,F,P).</code></pre>
<p>that takes 1000 samples where <code>biased(coin)</code> is true and returns in <code>S</code> the number of samples where <code>heads(coin)</code> is true, in <code>F</code> the number of samples where <code>heads(coin)</code> is false and in <code>P</code> the estimated probability (<code>S/1000</code>).</p>
<p>Differently from exact inference, in approximate inference the evidence can be a conjunction of atoms.</p>
<p>You can take a given number of sample with Metropolis-Hastings MCMC using</p>
<pre><code>mc_mh_sample(:Query:atom,:Evidence:atom,+Samples:int,+Lag:int,
  -Successes:int,-Failures:int,-Probability:float).</code></pre>
<p>where <code>Lag</code> is the number of sampled choices to forget before taking a new sample. For example (<a href="http://cplint.lamping.unife.it/example/inference/arithm.pl"><code>arithm.pl</code></a>)</p>
<pre><code>?- mc_mh_sample(eval(2,4),eval(1,3),10000,1,T,F,P).</code></pre>
<p>takes 10000 accepted samples and returns in <code>T</code> the number of samples where <code>eval(2,4)</code> is true, in <code>F</code> the number of samples where <code>eval(2,4)</code> is false and in <code>P</code> the estimated probability (<code>T/10000</code>).</p>
<p>Moreover, you can sample arguments of queries with rejection sampling and Metropolis-Hastings MCMC using</p>
<pre><code>mc_rejection_sample_arg(:Query:atom,:Evidence:atom,
  +Samples:int,?Arg:var,-Values:list).
mc_mh_sample_arg(:Query:atom,:Evidence:atom,
  +Samples:int,+Lag:int,?Arg:var,-Values:list).</code></pre>
<p>that return the distribution of values for <code>Arg</code> in <code>Query</code> in <code>Samples</code> of <code>Query</code> given that <code>Evidence</code> is true. The predicate returns in <code>Values</code> a list of couples <code>L-N</code> where <code>L</code> is the list of values of <code>Arg</code> for which <code>Query</code> succeeds in a world sampled at random where <code>Evidence</code> is true and <code>N</code> is the number of samples returning that list of values.</p>
<p>An example of use of the above predicates is</p>
<pre><code>?- mc_mh_sample_arg(eval(2,Y),eval(1,3),1000,1,Y,V).</code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/arithm.pl"><code>arithm.pl</code></a>.</p>
<p>Finally, you can compute expectations with</p>
<pre><code>mc_expectation(:Query:atom,+N:int,?Arg:var,-Exp:float).</code></pre>
<p>that computes the expected value of <code>Arg</code> in <code>Query</code> by sampling. It takes <code>N</code> samples of <code>Query</code> and sums up the value of <code>Arg</code> for each sample. The overall sum is divided by <code>N</code> to give <code>Exp</code>.</p>
<p>An example of use of the above predicate is</p>
<pre><code>?- mc_expectation(eventually(elect,T),1000,T,E).</code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/pctl_slep.pl"><code>pctl_slep.pl</code></a> that returns in <code>E</code> the expected value of <code>T</code> by taking 1000 samples.</p>
<p>To compute conditional expectations, use</p>
<pre><code>mc_mh_expectation(:Query:atom,:Evidence:atom,+N:int,
  +Lag:int,?Arg:var,-Exp:float).</code></pre>
<p>as in</p>
<pre><code>?- mc_mh_expectation(eval(2,Y),eval(1,3),1000,1,Y,E).</code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/arithm.pl"><code>arithm.pl</code></a> that computes the expectation of argument <code>Y</code> of <code>eval(2,Y)</code> given that <code>eval(1,3)</code> is true by taking 1000 samples using Metropolis-Hastings MCMC.</p>
<h2 id="condqcont">Conditional Queries on Continuous Variables</h2>
<p>When you have continuous random variables, you may be interested in sampling arguments of goals representing continuous random variables. In this way you can build a probability density of the sampled argument. When you do not have evidence or you have evidence on atoms not depending on continuous random variables, you can use the above predicates for sampling arguments.</p>
<p>For example</p>
<pre><code>?- mc_sample_arg(value(0,X),1000,X,L).</code></pre>
<p>from (<a href="http://cplint.lamping.unife.it/example/inference/gauss_mean_est.pl"><code>gauss_mean_est.pl</code></a>)) samples 1000 values for <code>X</code> in <code>value(0,X)</code> and returns them in <code>L</code>.</p>
<p>When you have evidence on ground atoms that have continuous values as arguments, you cannot use rejection sampling or Metropolis-Hastings, as the probability of the evidence is 0. For example, the probability of sampling a specific value from a Gaussian is 0. Continuous variables have probability densities instead of distributions as discrete variables. In this case, you can use likelihood weighting <span class="citation">[<a href="#ref-fung1990weighing">8</a>,<a href="#ref-koller2009probabilistic">10</a>,<a href="#ref-Nitti2016">12</a>]</span> to obtain samples of continuous arguments of a goal.</p>
<p>For each sample to be taken, likelihood weighting uses a meta-interpreter to find a sample where the goal is true, randomizing the choice of clauses when more than one resolves with the goal in order to obtain an unbiased sample. This meta-interpreter is similar to the one used to generate the first sample in Metropolis-Hastings.</p>
<p>Then a different meta-interpreter is used to evaluate the weight of the sample. This meta-interpreter starts with the evidence as the query and a weight of 1. Each time the meta-interpreter encounters a probabilistic choice over a continuous variable, it first checks whether a value has already been sampled. If so, it computes the probability density of the sampled value and multiplies the weight by it. If the value has not been sampled, it takes a sample and records it, leaving the weight unchanged. In this way, each sample in the result has a weight that is 1 for the prior distribution and that may be different from the posterior distribution, reflecting the influence of evidence.</p>
<p>The predicate</p>
<pre><code>mc_lw_sample(:Query:atom,:Evidence:atom,+Samples:int,-Prob:floar) is det</code></pre>
<p>samples <code>Query</code> a number of <code>Samples</code> times given that <code>Evidence</code> (a conjunction of atoms is allowed here). is true. The predicate returns in <code>Prob</code> the probability that the query is true. It performs likelihood weighting: each sample is weighted by the likelihood of evidence in the sample. For example</p>
<pre><code>?- mc_lw_sample(nation(a),student_gpa(4.0),1000,PPost).</code></pre>
<p>from <a href="http://cplint.lamping.unife.it/example/inference/indian_gpa.pl"><code>indian_gpa.pl</code></a> samples 1000 the query <code>nation(a)</code> given that <code>student_gpa(4.0)</code> has been observed.</p>
<p>The predicate</p>
<pre><code>mc_lw_sample_arg(:Query:atom,:Evidence:atom,+N:int,?Arg:var,-ValList)</code></pre>
<p>returns in <code>ValList</code> a list of couples <code>V-W</code> where <code>V</code> is a value of <code>Arg</code> for which <code>Query</code> succeeds and <code>W</code> is the weight computed by likelihood weighting according to <code>Evidence</code> (a conjunction of atoms is allowed here). For example</p>
<pre><code>?- mc_lw_sample_arg(value(0,X),(value(1,9),value(2,8)),100,X,L).</code></pre>
<p>from <a href="http://cplint.lamping.unife.it/example/inference/gauss_mean_est.pl"><code>gauss_mean_est.pl</code></a> samples 100 values for <code>X</code> in <code>value(0,X)</code> given that <code>value(1,9)</code> and <code>value(2,8)</code> have been observed.</p>
<h2 id="graphing">Graphing the Results</h2>
<p>in <code>cplint</code> on SWISH you can draw graphs for visualizing the results. There are two types of graphs: those that represent individual probability values with a bar chart and those that visualize the results of sampling arguments.</p>
<p>You can draw the probability of a query being true and being false as a bar chart with <code>prob_bar(:Query:atom,-Probability:dict)</code> as in</p>
<pre><code>?- prob_bar(heads(coin),P).</code></pre>
<p>if you include</p>
<pre><code>:- use_rendering(c3).</code></pre>
<p>before <code>:- pita.</code> <code>P</code> will be instantiated with a dict for rendering with <code>c3</code>. It will be shown as a bar chart with a bar for the probability of <code>heads(coin)</code> true and a bar for the probability of <code>heads(coin)</code> false.</p>
<p>When using <code>mcintyre</code>, you can use</p>
<pre><code>mc_prob_bar(:Query:atom,-Probability:dict).</code></pre>
<p>as in</p>
<pre><code>?- mc_prob_bar(heads(coin),P).</code></pre>
<p>to obtain a chart representation of the probability.</p>
<p>You can obtain a bar chart of the samples with</p>
<pre><code>?- mc_sample_bar(heads(coin),1000,Chart).</code></pre>
<p>that returns in <code>Chart</code> a diagram with one bar for the number of successes and one bar for the number of failures.</p>
<p>For visualizing the results of sampling arguments you can use</p>
<pre><code>mc_sample_arg_bar(:Query:atom,+Samples:int,?Arg:var,-Chart:dict).
mc_sample_arg_first_bar(:Query:atom,+Samples:int,
  ?Arg:var,-Chart:dict).
mc_rejection_sample_arg_bar(:Query:atom,:Evidence:atom,+Samples:int,
  ?Arg:var,-Chart:dict).
mc_mh_sample_arg_bar(:Query:atom,:Evidence:atom,+Samples:int,
  +Lag:int,?Arg:var,-Chart:dict).</code></pre>
<p>that return in <code>Chart</code> a bar chart with a bar for each possible sampled value whose size is the number of samples returning that value.</p>
<p>An example is</p>
<pre><code>?- mc_sample_arg_bar(reach(s0,0,S),50,S,Chart). </code></pre>
<p>of <a href="http://cplint.lamping.unife.it/example/inference/markov_chain.pl"><code>markov_chain.pl</code></a>.</p>
<p>Drawing a graph is particularly interesting when sampling values for continuous arguments of goals. In this case, you can use the samples to draw the probability density function of the argument. The predicate</p>
<pre><code>histogram(+List:list,+NBins:int,-Chart:dict) </code></pre>
<p>draws a histogram of the samples in <code>List</code> dividing the domain in <code>NBins</code> bins. <code>List</code> must be a list of couples of the form <code>[V]-W</code> where <code>V</code> is a sampled value and <code>W</code> is its weight. This is the format of the list of samples returned by argument sampling predicates except <code>mc_lw_sample_arg/5</code> that returns a list of couples <code>V-W</code>. In this case you can use</p>
<pre><code>densities(+PriorList:list,+PostList:list,+NBins:int,-Chart:dict)</code></pre>
<p>that draws a line chart of the density of two sets of samples, usually prior and post observations. The samples from the prior are in <code>PriorList</code> as couples <code>[V]-W</code>, while the samples from the posterior are in <code>PostList</code> as couples <code>V-W</code> where <code>V</code> is a value and <code>W</code> its weigth. The lines are drawn dividing the domain in <code>NBins</code> bins.</p>
<p>For example</p>
<pre><code>?-  mc_sample_arg(value(0,X),1000,X,L0),
    histogram(L0,40,Chart).</code></pre>
<p>from <a href="http://cplint.lamping.unife.it/example/inference/gauss_mean_est.pl"><code>gauss_mean_est.pl</code></a> takes 1000 samples of argument <code>X</code> of <code>value(0,X)</code> and draws the density of the samples using an histogram.</p>
<p>Instead</p>
<pre><code>?- mc_sample_arg(value(0,Y),1000,Y,L0),
   mc_lw_sample_arg(value(0,X),
    (value(1,9),value(2,8)),1000,X,L),
   densities(L0,L,NBins,Chart).</code></pre>
<p>from <a href="http://cplint.lamping.unife.it/example/inference/gauss_mean_est.pl"><code>gauss_mean_est.pl</code></a> takes 1000 samples of argument <code>X</code> of <code>value(0,X)</code> before and after observing <code>(value(1,9),value(2,8)</code> and draws the prior and posterior densities of the samples using a line chart.</p>
<h2 id="parameters">Parameters</h2>
<p>The inference modules have a number of parameters in order to control their behavior. They can be set with the directive</p>
<pre><code>:- set_pita(&lt;parameter&gt;,&lt;value&gt;).</code></pre>
<p>or</p>
<pre><code>:- set_mc(&lt;parameter&gt;,&lt;value&gt;).</code></pre>
<p>after initialization (<code>:-pita.</code> or <code>:-mc.</code>) but outside <code>:-begin/end_lpad.</code> The current value can be read with</p>
<pre><code>?- setting_pita(&lt;parameter&gt;,Value).</code></pre>
<p>or</p>
<pre><code>?- setting_mc(&lt;parameter&gt;,Value).</code></pre>
<p>from the top-level. The available parameters common to both <code>pita</code> and <code>mcintyre</code> are:</p>
<ul>
<li><p><code>epsilon_parsing</code>: if (1 - the sum of the probabilities of all the head atoms) is larger than <code>epsilon_parsing</code>, then <code>pita</code> adds the null event to the head. Default value <code>0.00001</code>.</p></li>
<li><p><code>single_var</code>: determines how non ground clauses are treated: if <code>true</code>, a single random variable is assigned to the whole non ground clause, if <code>false</code>, a different random variable is assigned to every grounding of the clause. Default value <code>false</code>.</p></li>
</ul>
<p>Moreover, <code>pita</code> has the parameters</p>
<ul>
<li><p><code>depth_bound</code>: if <code>true</code>, the depth of the derivation of the goal is limited to the value of the <code>depth</code> parameter. Default value <code>false</code>.</p></li>
<li><p><code>depth</code>: maximum depth of derivations when <code>depth_bound</code> is set to <code>true</code>. Default value <code>5</code>.</p></li>
</ul>
<p>If <code>depth_bound</code> is set to <code>true</code>, derivations are depth-bounded so you can query also programs containing infinite loops, for example programs where queries have an infinite number of explanations. However the probability that is returned is guaranteed only to be a lower bound, see for example <a href="http://cplint.lamping.unife.it/example/inference/markov_chaindb.pl"><code>markov_chaindb.pl</code></a></p>
<p><code>mcintyre</code> has the parameters</p>
<ul>
<li><p><code>min_error</code>: minimal width of the binomial proportion confidence interval for the probability of the query. When the confidence interval for the probability of the query is below <code>min_error</code>, the computation stops. Default value <code>0.01</code>.</p></li>
<li><p><code>k</code>: the number of samples to take before checking whether the the binomial proportion confidence interval is below <code>min_error</code>. Default value <code>1000</code>. <code>max_samples</code>: the maximum number of samples to take. This is used when the probability of the query is very close to 0 or 1. In fact <code>mcintyre</code> also checks for the validity of the the binomial proportion confidence interval: if less than 5 failures or successes are sampled, even if the width of the confidence interval is less than <code>min_error</code>, the computation continues. This would lead to non-termination in cases where the probability is 0 or 1. <code>max_samples</code> ensures termination. Default value <code>10e4</code>.</p></li>
</ul>
<p>The example <a href="http://cplint.lamping.unife.it/example/inference/markov_chain.pl"><code>markov_chain.pl</code></a> shows that <code>mcintyre</code> can perform inference in presence of an infinite number of explanations for the goal. Differently from <code>pita</code>, no depth bound is necessary, as the probability of selecting the infinite computation branch is 0. However, also <code>mcintyre</code> may not terminate if loops not involving probabilistic predicates are present.</p>
<p>If you want to set the seed of the random number generator, you can use SWI-Prolog predicates <code>setrand/1</code> and <code>getrand/1</code>, see <a href="http://www.swi-prolog.org/pldoc/doc_for?object=setrand/1">SWI-Prolog manual</a>.</p>
<h1 id="learning">Learning</h1>
<p>The following learning algorithms are available:</p>
<ul>
<li><p>EMBLEM (EM over Bdds for probabilistic Logic programs Efficient Mining): an implementation of EM for learning parameters that computes expectations directly on BDDs <span class="citation">[<a href="#ref-BelRig11-IDA-IJ">3</a>]</span>, <span class="citation">[<a href="#ref-BelRig11-CILC11-NC">1</a>]</span>, <span class="citation">[<a href="#ref-BelRig11-TR">2</a>]</span></p></li>
<li><p>SLIPCOVER (Structure LearnIng of Probabilistic logic programs by searChing OVER the clause space): an algorithm for learning the structure of programs by searching the clause space and the theory space separately <span class="citation">[<a href="#ref-BelRig13-TPLP-IJ">4</a>]</span></p></li>
</ul>
<h2 id="input">Input</h2>
<p>To execute the learning algorithms, prepare a Prolog file divided in five parts</p>
<ul>
<li><p>preamble</p></li>
<li><p>background knowledge, i.e., knowledge valid for all interpretations</p></li>
<li><p>LPAD/CPL-program for you which you want to learn the parameters (optional)</p></li>
<li><p>language bias information</p></li>
<li><p>example interpretations</p></li>
</ul>
<p>The preamble must come first, the order of the other parts can be changed.</p>
<p>For example, consider the Bongard problems of <span class="citation">[<a href="#ref-RaeLae95-ALT95">6</a>]</span>. <a href="http://cplint.lamping.unife.it/example/learning/bongard.pl"><code>bongard.pl</code></a> and <a href="http://cplint.lamping.unife.it/example/learning/bongardkeys.pl"><code>bongardkeys.pl</code></a> represent a Bongard problem. Let us consider <a href="http://cplint.lamping.unife.it/example/learning/bongard.pl"><code>bongard.pl</code></a>.</p>
<h3 id="preamble">Preamble</h3>
<p>In the preamble, the SLIPCOVER library is loaded with</p>
<pre><code>:- use_module(library(slipcover)).</code></pre>
<p>Now you can initialize SLIPCOVER with</p>
<pre><code>:- sc.</code></pre>
<p>At this point you can start setting parameters for SLIPCOVER such as for example</p>
<pre><code>:- set_sc(megaex_bottom,20).
:- set_sc(max_iter,2).
:- set_sc(max_iter_structure,5).
:- set_sc(verbosity,1).</code></pre>
<p>We will see later the list of available parameters. A particularly important parameter is <code>verbosity</code>: if set to 1, nothing is printed and learning is fastest, if set to 3 much information is printed and learning is slowest, 2 is in between. This ends the preamble.</p>
<h3 id="background-and-initial-lpadcpl-program">Background and Initial LPAD/CPL-program</h3>
<p>Now you can specify the background knowledge with a fact of the form</p>
<pre><code>bg(&lt;list of terms representing clauses&gt;).</code></pre>
<p>where the clauses must currently be deterministic. Alternatively, you can specify a set of clauses by including them in a section between <code>:- begin_bg.</code> and <code>:- end_bg.</code> For example</p>
<pre><code>:- begin_bg.
replaceable(gear).
replaceable(wheel).
replaceable(chain).
not_replaceable(engine).
not_replaceable(control_unit).
component(C):-
  replaceable(C).
component(C):-
  not_replaceable(C).
:- end_bg.</code></pre>
<p>from the <a href="http://cplint.lamping.unife.it/example/learning/mach.pl"><code>mach.pl</code></a> example. If you specify both a <code>bg/1</code> fact and a section, the clauses of the two will be combined.</p>
<p>Moreover, you can specify an initial program with a fact of the form</p>
<pre><code>in(&lt;list of terms representing clauses&gt;).</code></pre>
<p>The initial program is used in parameter learning for providing the structure. The indicated parameters do not matter as they are first randomized. Remember to enclose each clause in parentheses because <code>:-</code> has the highest precedence.</p>
<p>For example, <a href="http://cplint.lamping.unife.it/example/learning/bongard.pl"><code>bongard.pl</code></a> has the initial program</p>
<pre><code>in([(pos:0.197575 :-
       circle(A),
       in(B,A)),
    (pos:0.000303421 :-
       circle(A),
       triangle(B)), 
    (pos:0.000448807 :-
       triangle(A),
       circle(B))]).</code></pre>
<p>Alternatively, you can specify an input program in a section between <code>:- begin_in.</code> and <code>:- end_in.</code>, as for example</p>
<pre><code>:- begin_in.
pos:0.197575 :-
  circle(A),
  in(B,A).
pos:0.000303421 :-
  circle(A),
  triangle(B).
pos:0.000448807 :-
  triangle(A),
  circle(B).
:- end_in.</code></pre>
<p>If you specify both a <code>in/1</code> fact and a section, the clauses of the two will be combined.</p>
<h3 id="language-bias">Language Bias</h3>
<p>The language bias part contains the declarations of the input and output predicates. Output predicates are declared as</p>
<pre><code>output(&lt;predicate&gt;/&lt;arity&gt;).</code></pre>
<p>and indicate the predicate whose atoms you want to predict. Derivations for the atoms for this predicates in the input data are built by the system. These are the predicates for which new clauses are generated.</p>
<p>Input predicates are those whose atoms you are not interested in predicting. You can declare closed world input predicates with</p>
<pre><code>input_cw(&lt;predicate&gt;/&lt;arity&gt;).</code></pre>
<p>For these predicates, the only true atoms are those in the interpretations and those derivable from them using the background knowledge, the clauses in the input/hypothesized program are not used to derive atoms for these predicates. Moreover, clauses of the background knowledge that define closed world input predicates and that call an output predicate in the body will not be used for deriving examples.</p>
<p>Open world input predicates are declared with</p>
<pre><code>input(&lt;predicate&gt;/&lt;arity&gt;).</code></pre>
<p>In this case, if a subgoal for such a predicate is encountered when deriving a subgoal for the output predicates, both the facts in the interpretations, those derivable from them and the background knowledge, the background clauses and the clauses of the input program are used.</p>
<p>Then, you have to specify the language bias by means of mode declarations in the style of <a href="http://www.doc.ic.ac.uk/\string ~shm/progol.html">Progol</a>.</p>
<pre><code>modeh(&lt;recall&gt;,&lt;predicate&gt;(&lt;arg1&gt;,...)).</code></pre>
<p>specifies the atoms that can appear in the head of clauses, while</p>
<pre><code>modeb(&lt;recall&gt;,&lt;predicate&gt;(&lt;arg1&gt;,...)).</code></pre>
<p>specifies the atoms that can appear in the body of clauses. <code>&lt;recall&gt;</code> can be an integer or <code>*</code>. <code>&lt;recall&gt;</code> indicates how many atoms for the predicate specification are retained in the bottom clause during a saturation step. <code>*</code> stands for all those that are found. Otherwise the indicated number is randomly chosen.</p>
<p>Two specialization modes are available: <code>bottom</code> and <code>mode</code>. In the first, a bottom clause is built and the literals to be added during refinement are taken from it. In the latter, no bottom clause is built and the literals to be added during refinement are generated directly from the mode declarations.</p>
<p>Arguments of the form</p>
<pre><code>+&lt;type&gt;</code></pre>
<p>specifies that the argument should be an input variable of type <code>&lt;type&gt;</code>, i.e., a variable replacing a <code>+&lt;type&gt;</code> argument in the head or a <code>-&lt;type&gt;</code> argument in a preceding literal in the current hypothesized clause.</p>
<p>Another argument form is</p>
<pre><code>-&lt;type&gt;</code></pre>
<p>for specifying that the argument should be a output variable of type <code>&lt;type&gt;</code>. Any variable can replace this argument, either input or output. The only constraint on output variables is that those in the head of the current hypothesized clause must appear as output variables in an atom of the body.</p>
<p>Other forms are</p>
<pre><code>#&lt;type&gt;</code></pre>
<p>for specifying an argument which should be replaced by a constant of type <code>&lt;type&gt;</code> in the bottom clause but should not be used for replacing input variables of the following literals when building the bottom clause or</p>
<pre><code>-#&lt;type&gt;</code></pre>
<p>for specifying an argument which should be replaced by a constant of type <code>&lt;type&gt;</code> in the bottom clause and that should be used for replacing input variables of the following literals when building the bottom clause.</p>
<pre><code>&lt;constant&gt;</code></pre>
<p>for specifying a constant.</p>
<p>Note that arguments of the form <code>#&lt;type&gt;</code> <code>-#&lt;type&gt;</code> are not available in specialization mode <code>mode</code>, if you want constants to appear in the literals you have to indicate them one by one in the mode declarations.</p>
<p>An example of language bias for the Bongard domain is</p>
<pre><code>output(pos/0).

input_cw(triangle/1).
input_cw(square/1).
input_cw(circle/1).
input_cw(in/2).
input_cw(config/2).

modeh(*,pos).
modeb(*,triangle(-obj)).
modeb(*,square(-obj)).
modeb(*,circle(-obj)).
modeb(*,in(+obj,-obj)).
modeb(*,in(-obj,+obj)).
modeb(*,config(+obj,-#dir)).</code></pre>
<p>SLIPCOVER also requires facts for the <code>determination/2</code> predicate Aleph-style that indicate which predicates can appear in the body of clauses. For example</p>
<pre><code>determination(pos/0,triangle/1).
determination(pos/0,square/1).
determination(pos/0,circle/1).
determination(pos/0,in/2).
determination(pos/0,config/2).</code></pre>
<p>state that <code>triangle/1</code> can appear in the body of clauses for <code>pos/0</code>.</p>
<p>SLIPCOVER also allows mode declarations of the form</p>
<pre><code>modeh(&lt;r&gt;,[&lt;s1&gt;,...,&lt;sn&gt;],[&lt;a1&gt;,...,&lt;an&gt;],[&lt;P1/Ar1&gt;,...,&lt;Pk/Ark&gt;]). </code></pre>
<p>These mode declarations are used to generate clauses with more than two head atoms. In them, <code>&lt;s1&gt;,...,&lt;sn&gt;</code> are schemas, <code>&lt;a1&gt;,...,&lt;an&gt;</code> are atoms such that <code>&lt;ai&gt;</code> is obtained from <span class="math inline">\(\verb|&lt;si&gt;|\)</span> by replacing placemarkers with variables, <code>&lt;Pi/Ari&gt;</code> are the predicates admitted in the body. <code>&lt;a1&gt;,...,&lt;an&gt;</code> are used to indicate which variables should be shared by the atoms in the head. An example of such a mode declaration (from <code>uwcselearn.pl</code>) is</p>
<pre><code>modeh(*,
[advisedby(+person,+person),tempadvisedby(+person,+person)],
[advisedby(A,B),tempadvisedby(A,B)],
[professor/1,student/1,hasposition/2,inphase/2,
publication/2,taughtby/3,ta/3,courselevel/2,yearsinprogram/2]).</code></pre>
<p>If you want to specify negative literals for addition in the body of clauses, you should define a new predicate in the background as in</p>
<pre><code>not_worn(C):-
  component(C),
  \+ worn(C).
one_worn:-
  worn(_).
none_worn:-
  \+ one_worn.</code></pre>
<p>from <a href="http://cplint.lamping.unife.it/example/learning/mach.pl"><code>mach.pl</code></a> and add the new predicate in a <code>modeb/2</code> fact</p>
<pre><code>modeb(*,not_worn(-comp)).
modeb(*,none_worn).</code></pre>
<p>Note that successful negative literals do not instantiate the variables, so if you want a variable appearing in a negative literal to be an output variable you must instantiate before calling the negative literals. The new predicates must also be declared as input</p>
<pre><code>input_cw(not_worn/1).
input_cw(none_worn/0).</code></pre>
<p>Lookahead can also be specified with facts of the form</p>
<pre><code>lookahead(&lt;literal&gt;,&lt;list of literals&gt;).</code></pre>
<p>In this case when a literal matching <code>&lt;literal&gt;</code> is added to the body of clause during refinement, then also the literals matching <code>&lt;list of literals&gt;</code> will be added. An example of such declaration (from <a href="http://cplint.lamping.unife.it/example/learning/muta.pl"><code>muta.pl</code></a>) is</p>
<pre><code>lookahead(logp(_),[(_=_))]).</code></pre>
<p>Note that <code>&lt;list of literals&gt;</code> is copied with <code>copy_term/2</code> before matching, so variables in common between <code>&lt;literal&gt;</code> and <code>&lt;list of literals&gt;</code> may not be in common in the refined clause.</p>
<p>It is also possible to specify that a literal can only be added together with other literals with facts of the form</p>
<pre><code>lookahead_cons(&lt;literal&gt;,&lt;list of literals&gt;).</code></pre>
<p>In this case <code>&lt;literal&gt;</code> is added to the body of clause during refinement only together with literals matching <code>&lt;list of literals&gt;</code>. An example of such declaration is</p>
<pre><code>lookahead_cons(logp(_),[(_=_))]).</code></pre>
<p>Also here <code>&lt;list of literals&gt;</code> is copied with <code>copy_term/2</code> before matching, so variables in common between <code>&lt;literal&gt;</code> and <code>&lt;list of literals&gt;</code> may not be in common in the refined clause.</p>
<p>Moreover, we can specify lookahead with</p>
<pre><code>lookahead_cons_var(&lt;literal&gt;,&lt;list of literals&gt;).</code></pre>
<p>In this case <code>&lt;literal&gt;</code> is added to the body of clause during refinement only together with literals matching <code>&lt;list of literals&gt;</code> and <code>&lt;list of literals&gt;</code> is not copied before matching, so variables in common between <code>&lt;literal&gt;</code> and <code>&lt;list of literals&gt;</code> are in common also in the refined clause. This is allowed only with <code>specialization</code> set to <code>bottom</code>. An example of such declaration is</p>
<pre><code>lookahead_cons_var(logp(B),[(B=_))]).</code></pre>
<h3 id="example-interpretations">Example Interpretations</h3>
<p>The last part of the file contains the data. You can specify data with two modalities: models and keys. In the models type, you specify an example model (or interpretation or megaexample) as a list of Prolog facts initiated by <code>begin(model(&lt;name&gt;)).</code> and terminated by <code>end(model(&lt;name&gt;)).</code> as in</p>
<pre><code>begin(model(2)).
pos.
triangle(o5).
config(o5,up).
square(o4).
in(o4,o5).
circle(o3).
triangle(o2).
config(o2,up).
in(o2,o3).
triangle(o1).
config(o1,up).
end(model(2)).</code></pre>
<p>The interpretations may contain a fact of the form</p>
<pre><code>prob(0.3).</code></pre>
<p>assigning a probability (0.3 in this case) to the interpretations. If this is omitted, the probability of each interpretation is considered equal to <span class="math inline">\(1/n\)</span> where <span class="math inline">\(n\)</span> is the total number of interpretations. <code>prob/1</code> can be used to set a different multiplicity for the interpretations.</p>
<p>The facts in the interpretation are loaded in SWI-Prolog database by adding an extra initial argument equal to the name of the model. After each interpretation is loaded, a fact of the form <code>int(&lt;id&gt;)</code> is asserted, where <code>id</code> is the name of the interpretation. This can be used in order to retrieve the list of interpretations.</p>
<p>Alternatively, with the keys modality, you can directly write the facts and the first argument will be interpreted as a model identifier. The above interpretation in the keys modality is</p>
<pre><code>pos(2).
triangle(2,o5).
config(2,o5,up).
square(2,o4).
in(2,o4,o5).
circle(2,o3).
triangle(2,o2).
config(2,o2,up).
in(2,o2,o3).
triangle(2,o1).
config(2,o1,up).</code></pre>
<p>which is contained in the <a href="http://cplint.lamping.unife.it/example/learning/bongardkeys.pl"><code>bongardkeys.pl</code></a> This is also how model <code>2</code> above is stored in SWI-Prolog database. The two modalities, models and keys, can be mixed in the same file. Facts for <code>int/1</code> are not asserted for interpretations in the key modality but can be added by the user explicitly.</p>
<p>Note that you can add background knowledge that is not probabilistic directly to the file writing the clauses taking into account the model argument. For example (<code>carc.pl</code>) contains</p>
<pre><code>connected(_M,Ring1,Ring2):-
  Ring1 \= Ring2,
  member(A,Ring1),
  member(A,Ring2), !.

symbond(Mod,A,B,T):- bond(Mod,A,B,T).
symbond(Mod,A,B,T):- bond(Mod,B,A,T).</code></pre>
<p>where the first argument of all the atoms is the model.</p>
<p>Example <a href="http://cplint.lamping.unife.it/example/learning/registration.pl"><code>registration.pl</code></a> contains for example</p>
<pre><code>party(M,P):-
  participant(M,_, _, P, _).</code></pre>
<p>that defines intensionally the target predicate <code>party/1</code>. Here <code>M</code> is the model and <code>participant/4</code> is defined in the interpretations. You can also define intensionally the negative examples with</p>
<pre><code>neg(party(M,yes)):- party(M,no).
neg(party(M,no)):- party(M,yes).</code></pre>
<p>Then you must indicate how the examples are divided in folds with facts of the form: <code>fold(&lt;fold_name&gt;,&lt;list of model identifiers&gt;)</code>, as for example</p>
<pre><code>fold(train,[2,3,...]).
fold(test,[490,491,...]).</code></pre>
<p>As the input file is a Prolog program, you can define intensionally the folds as in</p>
<pre><code>fold(all,F):-
  findall(I,int(I),F).</code></pre>
<p><code>fold/2</code> is dynamic so you can also write (<a href="http://cplint.lamping.unife.it/example/learning/registration.pl"><code>registration.pl</code></a>)</p>
<pre><code>:- fold(all,F),
   sample(4,F,FTr,FTe),
   assert(fold(rand_train,FTr)),
   assert(fold(rand_test,FTe)).</code></pre>
<p>which however must be inserted after the input interpretations otherwise the facts for <code>int/1</code> will not be available and the fold <code>all</code> would be empty. This command uses <code>sample(N,List,Sampled,Rest)</code> exported from <code>slipcover</code> that samples <code>N</code> elements from <code>List</code> and returns the sampled elements in <code>Sampled</code> and the rest in <code>Rest</code>. If <code>List</code> has <code>N</code> elements or less, <code>Sampled</code> is equal to <code>List</code> and <code>Rest</code> is empty.</p>
<h2 id="commands">Commands</h2>
<h3 id="parameter-learning">Parameter Learning</h3>
<p>To execute EMBLEM, prepare an input file in the editor panel as indicated above and call</p>
<pre><code>?- induce_par(&lt;list of folds&gt;,P).</code></pre>
<p>where <code>&lt;list of folds&gt;</code> is a list of the folds for training and <code>P</code> will contain the input program with updated parameters.</p>
<p>For example <a href="http://cplint.lamping.unife.it/example/bongard.pl"><code>bongard.pl</code></a>, you can perform parameter learning on the <code>train</code> fold with</p>
<pre><code>?- induce_par([train],P).</code></pre>
<p>A program can also be tested on a test set with</p>
<pre><code>?- test(&lt;program&gt;,&lt;list of folds&gt;,LL,AUCROC,ROC,AUCPR,PR).</code></pre>
<p>where <code>&lt;program&gt;</code> is a list of terms representing clauses and <code>&lt;list of folds&gt;</code> is a list of folds. This returns the log likelihood of the test examples in <code>LL</code>, the Area Under the ROC curve in <code>AUCROC</code>, a dictionary containing the list of points (in the form of Prolog pairs <code>x-y</code>) of the ROC curve in <code>ROC</code>, the Area Under the PR curve in <code>AUCPR</code>, a dictionary containing the list of points of the PR curve in <code>PR</code>.</p>
<p>For example, to test on fold <code>test</code> the program learned on fold <code>train</code> you can run the query</p>
<pre><code>?- induce_par([train],P),
   test(P,[test],LL,AUCROC,ROC,AUCPR,PR).</code></pre>
<p>Or you can test the input program on the fold <code>test</code> with</p>
<pre><code>?- in(P),
   test(P,[test],LL,AUCROC,ROC,AUCPR,PR).</code></pre>
<p>By including</p>
<pre><code>:- use_rendering(c3).
:- use_rendering(lpad).</code></pre>
<p>in the code before <code>:- sc.</code> the curves will be shown as graphs and the output program will be pretty printed.</p>
<h3 id="structure-learning">Structure Learning</h3>
<p>To execute SLIPCOVER, prepare an input file in the editor panel as indicated above and call</p>
<pre><code>?- induce(&lt;list of folds&gt;,P).</code></pre>
<p>where <code>&lt;list of folds&gt;</code> is a list of the folds for training and <code>P</code> will contain the learned program.</p>
<p>For example <a href="http://cplint.lamping.unife.it/example/bongard.pl"><code>bongard.pl</code></a>, you can perform structure learning on the <code>train</code> fold with</p>
<pre><code>?- induce([train],P).</code></pre>
<p>A program can also be tested on a test set with <code>test/7</code> as described above.</p>
<h2 id="parameters-1">Parameters</h2>
<p>Parameters are set with commands of the form</p>
<pre><code>:- set_sc(&lt;parameter&gt;,&lt;value&gt;).</code></pre>
<p>The available parameters are:</p>
<ul>
<li><p><code>specialization</code>: (values: <code>{bottom,mode}</code>, default value: <code>bottom</code>) specialization mode.</p></li>
<li><p><code>depth_bound</code>: (values: <code>{true,false}</code>, default value: <code>true</code>) if <code>true</code>, the depth of the derivation of the goal is limited to the value of the <code>depth</code> parameter.</p></li>
<li><p><code>depth</code> (values: integer, default value: 2): depth of derivations if <code>depth_bound</code> is set to <code>true</code></p></li>
<li><p><code>single_var</code> (values: <code>{true,false}</code>, default value: <code>false</code>): if set to <code>true</code>, there is a random variable for each clause, instead of a different random variable for each grounding of each clause</p></li>
<li><p><code>epsilon_em</code> (values: real, default value: 0.1): if the difference in the log likelihood in two successive parameter EM iteration is smaller than <code>epsilon_em</code>, then EM stops</p></li>
<li><p><code>epsilon_em_fraction</code> (values: real, default value: 0.01): if the difference in the log likelihood in two successive parameter EM iteration is smaller than <code>epsilon_em_fraction</code>*(-current log likelihood), then EM stops</p></li>
<li><p><code>iter</code> (values: integer, defualt value: 1): maximum number of iteration of EM parameter learning. If set to -1, no maximum number of iterations is imposed</p></li>
<li><p><code>iterREF</code> (values: integer, defualt value: 1, valid for SLIPCOVER): maximum number of iteration of EM parameter learning for refinements. If set to -1, no maximum number of iterations is imposed.</p></li>
<li><p><code>random_restarts_number</code> (values: integer, default value: 1, valid for EMBLEM and SLIPCOVER): number of random restarts of parameter EM learning</p></li>
<li><p><code>random_restarts_REFnumber</code> (values: integer, default value: 1, valid for SLIPCOVER): number of random restarts of parameter EM learning for refinements</p></li>
<li><p><code>setrand</code> (values: rand(integer,integer,integer)): seed for the random functions, see SWI-Prolog manual for the allowed values</p></li>
<li><p><code>logzero</code> (values: negative real, default value <span class="math inline">\(\log(0.000001)\)</span>): value assigned to <span class="math inline">\(\log 0\)</span></p></li>
<li><p><code>max_iter</code> (values: integer, default value: 10, valid for SLIPCOVER): number of interations of beam search</p></li>
<li><p><code>max_var</code> (values: integer, default value: 4, valid for SLIPCOVER): maximum number of distinct variables in a clause</p></li>
<li><p><code>beamsize</code> (values: integer, default value: 100, valid for SLIPCOVER): size of the beam</p></li>
<li><p><code>megaex_bottom</code> (values: integer, default value: 1, valid for SLIPCOVER): number of mega-examples on which to build the bottom clauses</p></li>
<li><p><code>initial_clauses_per_megaex</code> (values: integer, default value: 1, valid for SLIPCOVER): number of bottom clauses to build for each mega-example (or model or interpretation)</p></li>
<li><p><code>d</code> (values: integer, default value: 1, valid for SLIPCOVER): number of saturation steps when building the bottom clause</p></li>
<li><p><code>max_iter_structure</code> (values: integer, default value: 10000, valid for SLIPCOVER): maximum number of theory search iterations</p></li>
<li><p><code>background_clauses</code> (values: integer, default value: 50, valid for SLIPCOVER): maximum numbers of background clauses</p></li>
<li><p><code>maxdepth_var</code> (values: integer, default value: 2, valid for SLIPCOVER): maximum depth of variables in clauses (as defined in <span class="citation">[<a href="#ref-DBLP:journals/ai/Cohen95">5</a>]</span>).</p></li>
<li><p><code>neg_ex</code> (values: <code>given</code>, <code>cw</code>, default value: <code>cw</code>): if set to <code>given</code>, the negative examples in testing are taken from the test folds interpretations, i.e., those examples <code>ex</code> stored as <code>neg(ex)</code>; if set to <code>cw</code>, the negative examples are generated according to the closed world assumption, i.e., all atoms for target predicates that are not positive examples. The set of all atoms is obtained by collecting the set of constants for each type of the arguments of the target predicate.</p></li>
<li><p><code>verbosity</code> (values: integer in [1,3], default value: 1): level of verbosity of the algorithms</p></li>
</ul>
<h1 id="download-query-results-through-an-api">Download Query Results through an API</h1>
<p>The results of queries can also be downloaded programmatically by directly approaching the Pengine API. Example client code is <a href="https://github.com/friguzzi/swish/tree/master/client">available</a>. For example, the <code>swish-ask.sh</code> client can be used with bash to download the results for a query in CSV. The call below downloads a CSV file for the coin example.</p>
<pre><code>$ bash swish-ask.sh --server=http://cplint.lamping.unife.it \   
  examples/inference/coin.pl Prob &quot;prob(heads(coin),Prob)&quot;</code></pre>
<p>The script can ask queries against Prolog scripts stored in <a href="http://cplint.lamping.unife.it" class="uri">http://cplint.lamping.unife.it</a> by specifying the script on the commandline. User defined files stored in <code>cplint</code> on SWISH (locations of type <a href="http://cplint.lamping.unife.it/p/coin_user.pl" class="uri">http://cplint.lamping.unife.it/p/coin_user.pl</a>) can be directly used, for example:</p>
<pre><code>$ bash swish-ask.sh --server=http://cplint.lamping.unife.it \
  coin_user.pl Prob &quot;prob(heads(coin),Prob)&quot;</code></pre>
<p>Example programs can be used by specifying the folder portion of the url of the example, as in the first coin example above where the url for the program is <a href="http://cplint.lamping.unife.it/examples/inference/coin.pl" class="uri">http://cplint.lamping.unife.it/examples/inference/coin.pl</a>.</p>
<p>Results can be downloaded in JSON using the option <code>--json-s</code> or <code>--json-html</code>. With the first the output is in a simple string format where Prolog terms are sent using quoted write, the latter serialize responses as HTML strings. E.g.</p>
<pre><code>$ bash swish-ask.sh --json-s --server=http://cplint.lamping.unife.it \
  coin_user.pl Prob &quot;prob(heads(coin),Prob)&quot;</code></pre>
<p>The JSON format can also be modified. See <a href="http://www.swi-prolog.org/pldoc/doc_for?object=pengines%3Aevent_to_json/4">http://www.swi-prolog.org/pldoc/doc_for?object=pengines%3Aevent_to_json/4</a>.</p>
<p>Prolog can exploit the Pengine API directly. For example, the above can be called as:</p>
<pre><code>?- [library(pengines)].
?- pengine_rpc(&#39;http://cplint.lamping.unife.it&#39;,
     prob(heads(coin),Prob),
     [ src_url(&#39;https://raw.githubusercontent.com/friguzzi/swish/\  
master/examples/inference/coin.pl&#39;),
     application(swish)
     ]).
Prob = 0.51.
?-</code></pre>
<h1 id="manual-in-pdf">Manual in PDF</h1>
<p>A PDF version of the manual is available at <a href="https://github.com/friguzzi/cplint/blob/master/doc/help-cplint.pdf" class="uri">https://github.com/friguzzi/cplint/blob/master/doc/help-cplint.pdf</a>.</p>
<h1 id="bibliography" class="unnumbered">Bibliography</h1>
<div id="refs" class="references">
<div id="ref-BelRig11-CILC11-NC">
<p>1. Elena Bellodi and Fabrizio Riguzzi. 2011. EM over binary decision diagrams for probabilistic logic programs. <em>Proceedings of the 26th italian conference on computational logic (CILC2011), pescara, italy, 31 august 31-2 september, 2011</em>. Retrieved from <a href="http://www.ing.unife.it/docenti/FabrizioRiguzzi/Papers/BelRig-CILC11.pdf" class="uri">http://www.ing.unife.it/docenti/FabrizioRiguzzi/Papers/BelRig-CILC11.pdf</a></p>
</div>
<div id="ref-BelRig11-TR">
<p>2. Elena Bellodi and Fabrizio Riguzzi. 2011. <em>EM over binary decision diagrams for probabilistic logic programs</em>. Dipartimento di Ingegneria, UniversitÃ  di Ferrara, Italy. Retrieved from <a href="http://www.unife.it/dipartimento/ingegneria/informazione/informatica/rapporti-tecnici-1/CS-2011-01.pdf/view" class="uri">http://www.unife.it/dipartimento/ingegneria/informazione/informatica/rapporti-tecnici-1/CS-2011-01.pdf/view</a></p>
</div>
<div id="ref-BelRig11-IDA-IJ">
<p>3. Elena Bellodi and Fabrizio Riguzzi. 2013. Expectation Maximization over binary decision diagrams for probabilistic logic programs. <em>Intelligent Data Analysis</em> 17, 2: 343â€“363. Retrieved from <a href="http://ds.ing.unife.it/~friguzzi/Papers/BelRig13-IDA-IJ.pdf" class="uri">http://ds.ing.unife.it/~friguzzi/Papers/BelRig13-IDA-IJ.pdf</a></p>
</div>
<div id="ref-BelRig13-TPLP-IJ">
<p>4. Elena Bellodi and Fabrizio Riguzzi. 2015. Structure learning of probabilistic logic programs by searching the clause space. <em>Theory and Practice of Logic Programming</em> 15, 2: 169â€“212. Retrieved from <a href="http://arxiv.org/abs/1309.2080" class="uri">http://arxiv.org/abs/1309.2080</a></p>
</div>
<div id="ref-DBLP:journals/ai/Cohen95">
<p>5. William W. Cohen. 1995. Pac-learning non-recursive prolog clauses. <em>Artif. Intell.</em> 79, 1: 1â€“38.</p>
</div>
<div id="ref-RaeLae95-ALT95">
<p>6. L. De Raedt and W. Van Laer. 1995. Inductive constraint logic. <em>Proceedings of the 6th conference on algorithmic learning theory (aLT 1995)</em>, Springer, 80â€“94.</p>
</div>
<div id="ref-DBLP:conf/ijcai/RaedtKT07">
<p>7. L. De Raedt, A. Kimmig, and H. Toivonen. 2007. ProbLog: A probabilistic Prolog and its application in link discovery. <em>International joint conference on artificial intelligence</em>, 2462â€“2467.</p>
</div>
<div id="ref-fung1990weighing">
<p>8. Robert M Fung and Kuo-Chu Chang. 1990. Weighing and integrating evidence for stochastic simulation in bayesian networks. <em>Fifth annual conference on uncertainty in artificial intelligence</em>, North-Holland Publishing Co., 209â€“220.</p>
</div>
<div id="ref-TLP:8688161">
<p>9. Muhammad Asiful Islam, CR Ramakrishnan, and IV Ramakrishnan. 2012. Inference in probabilistic logic programs with continuous random variables. <em>tplp_j</em> 12, Special Issue 4-5: 505â€“523. <a href="http://doi.org/10.1017/S1471068412000154" class="uri">http://doi.org/10.1017/S1471068412000154</a></p>
</div>
<div id="ref-koller2009probabilistic">
<p>10. D. Koller and N. Friedman. 2009. <em>Probabilistic graphical models: Principles and techniques</em>. MIT Press, Cambridge, MA.</p>
</div>
<div id="ref-nampally2014adaptive">
<p>11. Arun Nampally and CR Ramakrishnan. 2014. Adaptive mCMC-based inference in probabilistic logic programs. <em>arXiv preprint arXiv:1403.6036</em>. Retrieved from <a href="http://arxiv.org/pdf/1403.6036.pdf" class="uri">http://arxiv.org/pdf/1403.6036.pdf</a></p>
</div>
<div id="ref-Nitti2016">
<p>12. Davide Nitti, Tinne De Laet, and Luc De Raedt. 2016. Probabilistic logic programming for hybrid relational domains. <em>Mach. Learn.</em> 103, 3: 407â€“449. <a href="http://doi.org/10.1007/s10994-016-5558-8" class="uri">http://doi.org/10.1007/s10994-016-5558-8</a></p>
</div>
<div id="ref-DBLP:journals/ai/Poole97">
<p>13. David Poole. 1997. The independent choice logic for modelling multiple agents under uncertainty. <em>Artificial Intelligence</em> 94, 1-2: 7â€“56.</p>
</div>
<div id="ref-RigSwi10-ICLP10-IC">
<p>14. Fabrizio Riguzzi and Terrance Swift. 2010. Tabling and Answer Subsumption for Reasoning on Logic Programs with Annotated Disjunctions. <em>Technical communications of the international conference on logic programming</em>, Schloss Dagstuhlâ€“Leibniz-Zentrum fuer Informatik, 162â€“171. <a href="http://doi.org/10.4230/LIPIcs.ICLP.2010.162" class="uri">http://doi.org/10.4230/LIPIcs.ICLP.2010.162</a></p>
</div>
<div id="ref-Rig13-FI-IJ">
<p>15. Fabrizio Riguzzi. 2013. MCINTYRE: A Monte Carlo system for probabilistic logic programming. <em>Fundamenta Informaticae</em> 124, 4: 521â€“541. Retrieved from <a href="http://ds.ing.unife.it/~friguzzi/Papers/Rig13-FI-IJ.pdf" class="uri">http://ds.ing.unife.it/~friguzzi/Papers/Rig13-FI-IJ.pdf</a></p>
</div>
<div id="ref-Rig15-PLP-IW">
<p>16. Fabrizio Riguzzi. 2015. The distribution semantics is well-defined for all normal programs. <em>Proceedings of the 2nd international workshop on probabilistic logic programming (pLP)</em>, Sun SITE Central Europe, 69â€“84. Retrieved from <a href="http://ceur-ws.org/Vol-1413/#paper-06" class="uri">http://ceur-ws.org/Vol-1413/#paper-06</a></p>
</div>
<div id="ref-DBLP:conf/ijcai/SatoK97">
<p>17. Taisuke Sato and Yoshitaka Kameya. 1997. PRISM: A language for symbolic-statistical modeling. <em>International joint conference on artificial intelligence</em>, 1330â€“1339.</p>
</div>
<div id="ref-DBLP:journals/jair/SatoK01">
<p>18. Taisuke Sato and Yoshitaka Kameya. 2001. Parameter learning of logic programs for symbolic-statistical modeling. <em>J. Artif. Intell. Res.</em> 15: 391â€“454.</p>
</div>
<div id="ref-VenVer04-ICLP04-IC">
<p>19. J. Vennekens, S. Verbaeten, and M. Bruynooghe. 2004. Logic programs with annotated disjunctions. <em>International conference on logic programming</em>, Springer, 195â€“209.</p>
</div>
<div id="ref-von195113">
<p>20. John Von Neumann. 1951. Various techniques used in connection with random digits. <em>Nat. Bureau Stand. Appl. Math. Ser.</em> 12: 36â€“38.</p>
</div>
</div>
</body>
</html>
